{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca1dfb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import tarfile\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "import lxml.etree as etree\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "910d058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A few files from the Knesset dataset could not be parsed properly.\n",
    "We suspect that it is because then do not contain enough closing tags.\n",
    "Since those problematic files do not represent a huge fraction of the\n",
    "total dataset,  we preferred to set them aside instead of trying to\n",
    "edit them.\"\"\"\n",
    "\n",
    "def parse_corpus_file(fpath):\n",
    "    \"\"\"if fpath in ['./hebrew_dataset/16/'+pf for pf in problematic_files]:\n",
    "        return [], []\"\"\"\n",
    "    ns, vs = [], []\n",
    "    tree = ET.parse(fpath)\n",
    "    root = tree.getroot()\n",
    "    #print(root)\n",
    "    for paragraph in root[0]:\n",
    "        for sentence in paragraph:\n",
    "            for token in sentence:\n",
    "                #print(token.attrib['surface'])\n",
    "                for analysis in token:\n",
    "                    #print(analysis.attrib['score'])\n",
    "                    if 'score' in analysis.attrib.keys():\n",
    "                        if float(analysis.attrib['score']) > 0:\n",
    "                            for base in analysis:\n",
    "                                #if 'dottedLexiconItem' in base.attrib.keys():\n",
    "                                #print(base.attrib['dottedLexiconItem'])\n",
    "                                if 'lexiconItem' in base.attrib.keys():\n",
    "                                #print(base.attrib['lexiconItem'])\n",
    "                                    for pos in base:\n",
    "                                        if pos.tag == 'verb':\n",
    "                                            vs.append(base.attrib['lexiconItem'])\n",
    "                                        if pos.tag == 'noun':\n",
    "                                            ns.append(base.attrib['lexiconItem'])\n",
    "    return ns, vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c36fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_all_corpus_files():\n",
    "    \n",
    "    if 'knesset_tagged_16.tar.gz' not in os.listdir('./hebrew dataset/'):\n",
    "        url = 'https://yeda.cs.technion.ac.il:8443/corpus/software/corpora/knesset/tagged/knesset_tagged_16.tar.gz'\n",
    "        inp = './hebrew dataset/knesset_tagged_16.tar.gz'\n",
    "        response = wget.download(url, inp)\n",
    "        \n",
    "        f = tarfile.open(inp)\n",
    "        f.extractall('./hebrew dataset/')\n",
    "        f.close()\n",
    "\n",
    "    problematic_files = ['17892403.xml', '63021903.xml', '17880203.xml']\n",
    "    files = os.listdir('./hebrew dataset/16/')\n",
    "    files = [f for f in files if f[-4:] == '.xml']\n",
    "    n_files = len(files)\n",
    "    ns = []\n",
    "    vs = []\n",
    "    for i, f in enumerate(files):\n",
    "        print(f'Now processing {f}')\n",
    "        print(f'Step {i}/{n_files}')\n",
    "        if f in problematic_files:\n",
    "            continue\n",
    "        fpath = './hebrew dataset/16/'+f\n",
    "        new_ns, new_vs = parse_corpus_file(fpath)\n",
    "        ns.extend(new_ns)\n",
    "        vs.extend(new_vs)\n",
    "    unique_ns = list(set(ns))\n",
    "    unique_vs = list(set(vs))\n",
    "    print(f'Total number of nouns in the Knesset dataset: {len(ns)}')\n",
    "    print(f'Total number of verbs in the Knesset dataset: {len(vs)}')\n",
    "    print(f'Number of unique nouns in the Knesset dataset: {len(unique_ns)}')\n",
    "    print(f'Number of unique verbs in the Knesset dataset: {len(unique_vs)}')\n",
    "    return unique_ns, unique_vs\n",
    "\n",
    "\n",
    "def write_list_to_file(fname, l):\n",
    "    with open(f'./{fname}', 'w') as f:\n",
    "        for x in l:\n",
    "            f.write(x+'\\n')\n",
    "\n",
    "def load_list_from_file(fname):\n",
    "    with open(f'./{fname}', 'r') as f:\n",
    "        l = f.read().split('\\n')[:-1]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b45d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'unique_ns.csv' not in os.listdir('./hebrew dataset/') or 'unique_vs.csv' not in os.listdir('./hebrew dataset/'):\n",
    "    unique_ns, unique_vs = parse_all_corpus_files()\n",
    "    write_list_to_file('./hebrew dataset/unique_ns.csv', unique_ns)\n",
    "    write_list_to_file('./hebrew dataset/unique_vs.csv', unique_vs)\n",
    "    os.remove('./hebrew dataset/knesset_tagged_16.tar.gz')\n",
    "    shutil.rmtree('./hebrew dataset/16')\n",
    "else:\n",
    "    unique_ns = load_list_from_file('./hebrew dataset/unique_ns.csv')\n",
    "    unique_vs = load_list_from_file('./hebrew dataset/unique_vs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06556197",
   "metadata": {},
   "source": [
    "The cell below defines the patterns used to generate our data form Hebrew nouns. We are using two denominal, two non-denominal, and two \"other\" patterns. So, for each noun, we'll be able to generate at most 6 related forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af9625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self.datapoints = []\n",
    "        \n",
    "    def add_datapoint(self, datapoint):\n",
    "        self.datapoints.append(datapoint)\n",
    "    \n",
    "    def remove_datapoint(self, datapoint):\n",
    "        self\n",
    "        \n",
    "class Datapoint:\n",
    "    def __init__(self, noun, denoms, non_denoms, others):\n",
    "        self.noun = noun\n",
    "        self.denoms = denoms\n",
    "        self.non_denoms = non_denoms\n",
    "        self.others = others\n",
    "        \n",
    "class Template:\n",
    "    def __init__(self, label, category, content):\n",
    "        self.label = label\n",
    "        self.category = category\n",
    "        self.content = content\n",
    "        self.compat = []\n",
    "    \n",
    "    def add_compat(self, template):\n",
    "        self.compat.append(template)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe46cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nts = ['תץץוץת', 'ץץץן', 'מץץץ', 'תץץיץ', 'תץץוץ', 'ץץץון', 'מץץוץת', 'שץץץת', 'אץץץה', 'אץץוץ', 'ץץץני', 'מץץץת', 'מץץוץ', 'תץץוץה', 'אץץץ', 'אץץוץ']\n",
    "n_templates = []\n",
    "for nt in nts:\n",
    "    n_template = Template('', 'n', nt)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4e75a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_templates = ['תץץוץת', 'ץץץן', 'מץץץ', 'תץץיץ', 'תץץוץ', 'ץץץון', 'מץץוץת', 'שץץץת', 'אץץץה', 'אץץוץ', 'ץץץני', 'מץץץת', 'מץץוץ', 'תץץוץה', 'אץץץ', 'אץץוץ']\n",
    "\n",
    "dn_patterns = defaultdict(list)\n",
    "ndn_patterns = defaultdict()\n",
    "other_patterns = defaultdict()\n",
    "\n",
    "\n",
    "# pi'el pattern for denominal\n",
    "dn_patterns[0] = ['לתץץץ', 'לץץץן', 'למץץץ', 'לתץץץ', 'לתץץץ', 'לץץץן', 'למץץץ', 'לשץץץ', 'לאץץץ', 'לאץץץ', 'לץץץן', 'למץץץ', 'למץץץ', 'לתץץץ', 'לאץץץ', 'לאץץץ']\n",
    "\n",
    "# pi'el/pu'al pattern for non-denominal\n",
    "ndn_patterns[0] = 'לץץץ'\n",
    "\n",
    "# hitpa'el pattern for denominal\n",
    "dn_patterns[1] = ['NA', 'להתץץץן', 'להתמץץץ', 'NA', 'NA', 'להתץץץן', 'להתמץץץ', 'להשתץץץ', 'להתאץץץ', 'להתאץץץ', 'להתץץץן', 'להתמץץץ', 'להתמץץץ', 'NA', 'להתאץץץ', 'להתאץץץ']\n",
    "\n",
    "# hitpa'el pattern for non-denominal\n",
    "ndn_patterns[1] = 'להתץץץ'\n",
    "\n",
    "# kal/nif'al pattern\n",
    "other_patterns[0] = 'לץץוץ'\n",
    "\n",
    "# hif'il/huf'al pattern\n",
    "other_patterns[1] = 'להץץיץ'\n",
    "\n",
    "n_template_to_v_templates = defaultdict(lambda: defaultdict(str))\n",
    "\n",
    "for i, n in enumerate(n_templates):\n",
    "    n_template_to_v_templates[n]['denom'] = [dn_patterns[j][i] for j in range(2)]\n",
    "    n_template_to_v_templates[n]['non_denom'] = [ndn_patterns[j] for j in range(2)]\n",
    "    n_template_to_v_templates[n]['others'] = [other_patterns[j] for j in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e88f4b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def generate_dataset(n_template_to_v_templates, unique_ns):\n",
    "    \"\"\" This function generates datapoints using the Knesset annotated corpus.\n",
    "    First, it identifies nouns in the corpus that match the noun templates\n",
    "    specified above. The characters of the matched nouns that do not beong to \n",
    "    thew template are then retained, to generate denominals, non-denominal verbs,\n",
    "    and other verbs following the relevant patterns.\"\"\"\n",
    "\n",
    "    dataset = []\n",
    "    matched_ns = []\n",
    "    n_templates = n_template_to_v_templates.keys()\n",
    "\n",
    "    for n in unique_ns: # we go through the noun lexicon\n",
    "        all_matched_templates, all_skipped_characters = find_template(n, n_templates)\n",
    "        if not all_matched_templates: # no matching template found\n",
    "            continue\n",
    "\n",
    "        for n_template, skipped_characters in zip(all_matched_templates, all_skipped_characters):\n",
    "            datapoint = defaultdict(list)\n",
    "            datapoint['noun'] = n\n",
    "\n",
    "            for key in ['denom', 'non_denom', 'others']: # building the datapoint\n",
    "                datapoint[key] = []\n",
    "                for verbal_template in n_template_to_v_templates[n_template][key]:\n",
    "                    #print(\"verb template\", verbal_template)\n",
    "                    if verbal_template != 'NA':\n",
    "                        n_verbalized = ''\n",
    "                        i = 0\n",
    "                        for character in verbal_template: # producing the verbal form\n",
    "                            #print(\"n verb\", n_verbalized)\n",
    "                            if character == \"ץ\":\n",
    "                                n_verbalized+=skipped_characters[i]\n",
    "                                i += 1\n",
    "                            else:\n",
    "                                n_verbalized+=character\n",
    "                        #print(\"n verb\", n_verbalized)\n",
    "                        datapoint[key].append(n_verbalized)\n",
    "                    else:\n",
    "                        datapoint[key].append('')\n",
    "            dataset.append(datapoint)\n",
    "    return dataset\"\"\"\n",
    "\n",
    "def find_template(n, n_templates, cat='noun'):\n",
    "    all_matched_templates = []\n",
    "    all_skipped_characters = []\n",
    "    for n_template in n_templates:\n",
    "        if len(n) == len(n_template):\n",
    "            empty_positions = [_.start() for _ in re.finditer(\"ץ\", n_template)]\n",
    "            skipped_characters = [n[i] for i in empty_positions]\n",
    "\n",
    "            if cat == 'noun':\n",
    "                if 'ו' in skipped_characters or 'י' in skipped_characters:\n",
    "                    continue\n",
    "            \n",
    "            cut_template = ''.join([n_template[i] for i in range(len(n_template)) if i not in empty_positions])\n",
    "            cut_noun = ''.join([n[i] for i in range(len(n_template)) if i not in empty_positions])\n",
    "            if cut_template == cut_noun: # the template matches the noun:\n",
    "                all_matched_templates.append(n_template)\n",
    "                all_skipped_characters.append(skipped_characters)\n",
    "    return all_matched_templates, all_skipped_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n_template_to_v_templates, unique_ns):\n",
    "    \"\"\" This function generates datapoints using the Knesset annotated corpus.\n",
    "    First, it identifies nouns in the corpus that match the noun templates\n",
    "    specified above. The characters of the matched nouns that do not beong to \n",
    "    thew template are then retained, to generate denominals, non-denominal verbs,\n",
    "    and other verbs following the relevant patterns.\"\"\"\n",
    "\n",
    "    dataset = Dataset()\n",
    "    matched_ns = []\n",
    "    n_templates = n_template_to_v_templates.keys()\n",
    "\n",
    "    for n in unique_ns: # we go through the noun lexicon\n",
    "        all_matched_templates, all_skipped_characters = find_template(n, n_templates)\n",
    "        if not all_matched_templates: # no matching template found\n",
    "            continue\n",
    "\n",
    "        for n_template, skipped_characters in zip(all_matched_templates, all_skipped_characters):\n",
    "            datapoint = Datapoint(n, [], [], [])\n",
    "            #datapoint['noun'] = n\n",
    "\n",
    "            for key in ['denom', 'non_denom', 'others']: # building the datapoint\n",
    "                #datapoint[key] = []\n",
    "                for verbal_template in n_template_to_v_templates[n_template][key]:\n",
    "                    #print(\"verb template\", verbal_template)\n",
    "                    if verbal_template != 'NA':\n",
    "                        n_verbalized = ''\n",
    "                        i = 0\n",
    "                        for character in verbal_template: # producing the verbal form\n",
    "                            #print(\"n verb\", n_verbalized)\n",
    "                            if character == \"ץ\":\n",
    "                                n_verbalized+=skipped_characters[i]\n",
    "                                i += 1\n",
    "                            else:\n",
    "                                n_verbalized+=character\n",
    "                        #print(\"n verb\", n_verbalized)\n",
    "                        datapoint[key].append(n_verbalized)\n",
    "                    \"\"\"else:\n",
    "                        datapoint[key].append('')\"\"\"\n",
    "            dataset.add_datapoint(datapoint)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77a45c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of generated datapoints: 788\n",
      "Total number of generated denominals: 1435\n"
     ]
    }
   ],
   "source": [
    "dataset = generate_dataset(n_template_to_v_templates, unique_ns)\n",
    "print(f'Number of generated datapoints: {len(dataset)}')\n",
    "denoms = [datapoint['denom'] for datapoint in dataset]\n",
    "denoms = [denom for denom_pair in denoms for denom in denom_pair]\n",
    "denoms = [denom for denom in denoms if denom != '']\n",
    "print(f'Total number of generated denominals: {len(denoms)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f81c8ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(dataset, unique_vs):\n",
    "    \"\"\"This function eliminates the datapoints that do not even remotely\n",
    "    exist in the Knesset dataset. In particular, it checks if the generated\n",
    "    denominals, or any of their inflected forms, are present in the Knesset\n",
    "    corpus. If not, the datapoint is moved from the dataset to a special junk\n",
    "    list.\"\"\"\n",
    "\n",
    "    filtered_dataset, junk = copy.deepcopy(dataset), []\n",
    "    for i, datapoint in enumerate(dataset):\n",
    "        #print('Datapoint')\n",
    "        #print(datapoint)\n",
    "        for j, denominal in enumerate(datapoint['denom']):\n",
    "            #print(f'Now testing {denominal}')\n",
    "            if denominal == '':\n",
    "                #print('Denominal is null')\n",
    "                #print('Updated datapoint:')\n",
    "                filtered_dataset = remove_dn(filtered_dataset, denominal, i)\n",
    "                #print(filtered_dataset[i])\n",
    "            elif not is_a_verb(denominal, unique_vs, j):\n",
    "                #print('Denominal is not a verb')\n",
    "                junk.append(split_dn(copy.deepcopy(datapoint), denominal))\n",
    "                filtered_dataset = remove_dn(filtered_dataset, denominal, i)\n",
    "                #print('Updated datapoint:')\n",
    "                #print(filtered_dataset[i])\n",
    "                #print('Junk entry:')\n",
    "                #print(junk[-1])\n",
    "    filtered_dataset = [datapoint for datapoint in filtered_dataset if datapoint['denom']]\n",
    "    return filtered_dataset, junk\n",
    "\n",
    "def is_a_verb(dn, unique_vs, j):\n",
    "    inflected_dn = dn[1:]\n",
    "    if inflected_dn in unique_vs:\n",
    "        return True\n",
    "    if j == 0: # first denominal form\n",
    "        inflected_dn = dn[0]+'י'+dn[1:]\n",
    "        if inflected_dn in unique_vs:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def split_dn(datapoint, dn):\n",
    "    split_datapoint = datapoint\n",
    "    split_datapoint['denom'] = [dn]\n",
    "    return split_datapoint\n",
    "\n",
    "def remove_dn(dataset, dn, i):\n",
    "    datapoint = dataset[i]\n",
    "    datapoint['denom'].remove(dn)\n",
    "    dataset[i] = datapoint\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cda9a77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of remaining datapoints: 99\n",
      "Number of junk datapoints: 1322\n",
      "Total number of generated denominals: 113\n",
      "Total number of junk denominals: 1322\n"
     ]
    }
   ],
   "source": [
    "filtered_dataset, junk = filter_dataset(dataset, unique_vs)\n",
    "print(f'Number of remaining datapoints: {len(filtered_dataset)}')\n",
    "print(f'Number of junk datapoints: {len(junk)}')\n",
    "denoms = [datapoint['denom'] for datapoint in filtered_dataset]\n",
    "denoms = [denom for denom_pair in denoms for denom in denom_pair]\n",
    "denoms = [denom for denom in denoms if denom != '']\n",
    "print(f'Total number of generated denominals: {len(denoms)}')\n",
    "denoms = [datapoint['denom'] for datapoint in junk]\n",
    "denoms = [denom for denom_pair in denoms for denom in denom_pair]\n",
    "denoms = [denom for denom in denoms if denom != '']\n",
    "print(f'Total number of junk denominals: {len(denoms)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ffedc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataset(dataset, name):\n",
    "     with open(f'./{name}', 'w') as f:\n",
    "        f.write('Noun,Denominal,Non-denominal,Others\\n')\n",
    "        for datapoint in dataset:\n",
    "            f.write(datapoint['noun']+',')\n",
    "            for key in ['denom', 'non_denom', 'others']:\n",
    "                forms = datapoint[key]\n",
    "                for form in forms:\n",
    "                    f.write(form+' ')\n",
    "                if key != 'others':\n",
    "                    f.write(',')\n",
    "                else:\n",
    "                    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5e76d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataset(junk, './hebrew dataset/junk.csv')\n",
    "write_dataset(filtered_dataset, './hebrew dataset/filtered_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad255d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7ce9c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b52697a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e13a7c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = Datapoint('bla', ['bli', 'blu'], ['bly'], ['ble'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a1ad7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.add_datapoint(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "50665a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Datapoint at 0x7f25fc043040>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ddd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
